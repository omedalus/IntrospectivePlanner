{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Planner Design\n",
    "We'll build a traditional searching planning algorithm, the kind used in very old game players. It's a place to start. We'll call it a \"Monte Carlo\" planner because we'll probably expand neither breadth-first nor depth-first, but stochastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object model\n",
    "A very rudimentary object model that we can use before delving into a toy problem to solve. We should remember that these objects represent what the AI *believes* about the world, not what is actually true in the world.\n",
    "\n",
    "Here's the 30,000-ft view.\n",
    "* The *Environment* is the \"objective reality\" in which the Agent lives. \n",
    "* An *Agent* is the organism/entity/being that devises a *Plan*. \n",
    "  * An Agent exists within the context of an *Environment*.\n",
    "  * An Agent has a current *Frame*.\n",
    "  * An Agent can command the effecting of certain specific changes in that Environment by way of *Effectors*.\n",
    "  * An Agent can receive updates to its *Frame* by way of *Sensors*.\n",
    "  * An Agent can receive a *Reward* from the *Environment*.\n",
    "  * The Agent's job is to try to maximize this Reward.\n",
    "* A *Frame* is a summary of the Agent's current belief about the state of the Environment. \n",
    "  * A Frame doesn't have to *accurately* represent the entire Environment. It only needs to be accurate enough to increase Reward above some more naive baseline. The complexity of a Frame is programmatically arbitrary, and systems of higher or lesser intelligence can be built by experimenting with different Frame structures.\n",
    "  * A Frame's internal state gets set by the Agent after the Agent executes an Action. Upon executing an Action, the Agent consults its Sensors and sets the Frame's state accordingly.\n",
    "  * A Frame must include the Reward received by the Agent upon executing an Action that resulted in entering that Frame.\n",
    "    * The Reward can simply be an expected value, or it can be a magnitude combined with a probability.\n",
    "  * A Frame contains a set of *Actions*. Each Action is essentially mutually exclusive to all others, i.e. only one Action can be executed at a time.\n",
    "  * Multiple Frames can collapse into one another. That is, Frames can be observed to be equivalent, by virtue of representing equivalent states.\n",
    "* A *Plan* is an object that describes a sequence of *Actions*, and resulting *Consequence* Frames.\n",
    "  * The *Plan* always starts with the current *Frame*.\n",
    "* An *Action* represents one or more commands sent to the Effectors.\n",
    "  * An Action contains one or more *Consequences*.\n",
    "  * An Action has an expected Reward value, based on some function of the expected Rewards of the states of its Consequences. Presumably, this function is simply a weighted sum.\n",
    "  * It may be useful for the Action's Reward value to include a measurement of uncertainty.\n",
    "* A *Consequence* represents the result of committing an action within a Frame.\n",
    "  * A Consequence has a probability.\n",
    "  * A Consequence has a child Frame.\n",
    "* Eventually, defining new Actions or new compilations of Frame states will count as Actions in and of themselves, thus facilitating high-order learning. But that depends on exactly how the Frame represents the state. For now, Action and Frame definitions will be fixed, and supplied by the programmer.\n",
    "* The Agent contains an *ActionGenerator* that, given a Frame, randomly produces one or more Actions.\n",
    "* The Agent contains a *ConsequenceGenerator* that, given a Frame and an Action, randomly produces one or more Consequences.\n",
    "  * Upon producing these Consequences, the expected Reward of the Action may be updated.\n",
    "  * Upon updating the Reward of the Action, the ConsequenceGenerator may update the expected Reward of the Frame that the Action came from. The Frame's Reward consists not only of an intrinsic Reward native to that Frame, but also a supplemental Reward consisting of the expected Reward of the best Action one can perform within that Frame. This way, a small temporary gain may be offset by horrible consequences later, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frames\n",
    "A frame represents a currently or hypothetically true world state. A frame affords the performance of actions. Performing an action within the context of a frame can generate child frames, that contain new currently true facts. Each frame has a desirability score, and the planner's job is to maximize said score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame:\n",
    "    def __init__(self):\n",
    "        self.worldState = None\n",
    "        \n",
    "        # A collection of actions that can lead us to this frame.\n",
    "        # Used for tailing back through a Plan in order to report a victory route\n",
    "        # once a win condition is discovered.\n",
    "        self.fromActions = {}\n",
    "        \n",
    "        # The action collection represents Action objects, indexed by their keys.\n",
    "        self.actions = {}\n",
    "        \n",
    "    # Compute a lookup key for this Frame, presumably based on the worldState.\n",
    "    def key(self):\n",
    "        return ''\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Generator\n",
    "The Action Generator can generate actions within the context of a frame. It generates a new random action every time it's called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-40a42313e6da>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-40a42313e6da>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    class ActionGenerator:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Action:\n",
    "\n",
    "class ActionGenerator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generateAction(self, frame):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consequence Generator\n",
    "Given a frame and an action, the Consequence Generator generates a new frame that may result from performing that action within that frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsequenceGenerator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generateConsequence(self, frame, action):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-909ad46f5b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrndstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascii_letters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrndstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "rndstr = ''.join([random.choice(string.ascii_letters + string.digits) for n in xrange(32)])\n",
    "print(rndstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
